% !TEX root = ../optim-ml/OptimML.tex


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multi-Layers Perceptron}

In this section, we study the simplest example of non-linear parametric models, namely Multi-Layers Perceptron (MLP) with a single hidden layer (so they have in total 2 layers). Perceptron (with no hidden layer) corresponds to the linear models studied in the previous sections. MLP with more layers are obtained by stacking together several such simple MLP, and are studied in Section~\ref{sec-autodiff-mlp}, since the computation of their derivatives is very suited to automatic-differentiation methods.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MLP and its derivative}

The basic MLP $a \mapsto h_{W,u}(a)$ takes as input a feature vector $a \in \RR^p$, computes an intermediate hidden representation $b = Wa \in \RR^q$ using $q$ ``neurons'' stored as the rows $w_k \in \RR^p$ of the weight matrix $W \in \RR^{q \times p}$, passes these through a non-linearity $\rho : \RR \rightarrow \RR$, i.e. $\rho(b)=(\rho(b_k))_{k=1}^q$ and then outputs a scalar value as a linear combination with output weights $u \in \RR^q$, i.e. 
\eq{
	h_{W,u}(a) = \dotp{\rho(Wa)}{u} = \sum_{k=1}^q u_k \rho( (W a)_k ) = \sum_{k=1}^q u_k \rho(\dotp{a}{w_k}).
}
This function $h_{W,u}(\cdot)$ is thus a weighted sum of $q$ ``ridge functions'' $\rho(\dotp{\cdot}{w_k})$. These functions are constant in the direction orthogonal to the neuron $w_k$ and have a profile defined by $\rho$.

The most popular non-linearities are sigmoid functions such as
\eq{
	\rho(r) = \frac{e^r}{1+e^r}
	\qandq
	\rho(r) = \frac{1}{\pi} \text{atan}(r) + \frac{1}{2}
}
and the rectified linear unit (ReLu) function $\rho(r)=\max(r,0)$.

One often add a bias term in these models, and consider functions of the form $\rho(\dotp{\cdot}{w_k}+z_k)$ but this bias term can be integrated in the weight as usual by considering  $(\dotp{a}{w_k}+z_k = \dotp{(a,1)}{(w_k,z_k)}$, so we ignore it in the following section. This simply amount to replacing $a \in \RR^p$ by $(a,1) \in \RR^{p+1}$ and adding a dimension $p \mapsto p+1$, as a pre-processing of the features.

\paragraph{Expressiveness. }

In order to define function of arbitrary complexity when $q$ increases, it is important that $\rho$ is non-linear. Indeed, if $\rho(s)=s$, then $h_{W,u}(a) = \dotp{Wa}{u} = \dotp{a}{W^\top u}$. It is thus a linear function with weights $W^\top u$, whatever the number $q$ of neurons.  
%
Similarly, if $\rho$ is a polynomial on $\RR$ of degree $d$, then $h_{W,u}(\cdot)$ is itself a polynomial of degree $d$ in $\RR^p$, which is a linear space $V$ of finite dimension $\dim(V) = O(p^d)$. So even if $q$ increases, the dimension $\dim(V)$ stays fixed and  $h_{W,u}(\cdot)$ cannot approximate an arbitrary function outside $V$. 
%
In sharp contrast, one can show that if $\rho$ is not polynomial, then $h_{W,u}(\cdot)$ can approximate any continuous function, as studied in Section~\ref{sec-universality}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MLP and Gradient Computation}

Given pairs of features and data values $(a_i,y_i)_{i=1}^n$, and as usual storing the features in the rows of $A \in \RR^{n \times p}$, we consider the following least square regression function (similar computation can be done for classification losses)
\eq{
	\umin{x=(W,u)} f(W,u) \eqdef \frac{1}{2}\sum_{i=1}^n ( h_{W,u}(a_i)-y_i )^2
	= \frac{1}{2}\norm{\rho(AW^\top) u - y}^2.
}
Note that here, the parameters being optimized are $(W,u) \in \RR^{q \times p} \times \RR^q$.

%%%%
\paragraph{Optimizing with respect to $u$.}

This function $f$ is convex with respect to $u$, since it is a quadratic function. Its gradient with respect to $u$ can be computed as in~\eqref{eq-grad-ls} and thus
\eq{
	\nabla_u f(W,u) = \rho(AW^\top)^\top ( \rho(AW^\top) u - y )
}
and one can compute in closed form the solution (assuming $\ker(\rho(AW^\top))=\{0\}$) as 
\eq{
	u^\star = [ \rho(AW^\top)^\top \rho(AW^\top) ]^{-1} \rho(AW^\top)^\top y
		= 
		[ \rho(W A^\top) \rho(AW^\top) ]^{-1} \rho(W A^\top) y
}
When $W=\Id_p$ and $\rho(s)=s$ one recovers the least square formula~\eqref{eq-sol-leastsquare}.

%%%%
\paragraph{Optimizing with respect to $W$.}

The function $f$ is non-convex with respect to $W$ because the function $\rho$ is itself non-linear. 
%
Training a MLP is thus a delicate process, and one can only hope to obtain a local minimum of $f$. It is also important to initialize correctly the neurons $(w_k)_k$ (for instance as unit norm random vector, but bias terms might need some adjustment), while $u$ can be usually initialized at $0$.

To compute its gradient with respect to $W$, we first note that for a perturbation $\epsilon \in \RR^{q \times p}$, one has 
\eq{
	\rho(A (W+\epsilon)^\top ) = \rho(AW^\top + A\epsilon^\top) = \rho(AW^\top) + \rho'(AW^\top) \odot (A\epsilon^\top)
}
where we have denoted ``$\odot$'' the entry-wise multiplication of matrices, i.e. $U \odot V = (U_{i,j} V_{i,j})_{i,j}$.
%
One thus has, 
\begin{align*}
	f(W+\epsilon,u) &= \frac{1}{2}\norm{e+[\rho'(AW^\top)\odot(A\epsilon^\top)] y}^2
		\qwhereq e \eqdef \rho(AW^\top) u - y \in \RR^n \\
		&= f(W,u) + \dotp{e}{[\rho'(AW^\top)\odot(A\epsilon^\top)] y} + o(\norm{\epsilon}) \\
		&= f(W,u) + \dotp{A \epsilon^\top}{\rho'(AW^\top)\odot (e u^\top)} \\
		&= f(W,u) + \dotp{\epsilon^\top}{A^\top \times [\rho'(AW^\top)\odot (e u^\top)]}.
\end{align*}
The gradient thus reads
\eq{
	\nabla_W f(W,u) = [ \rho'(W A^\top)\odot (u e^\top) ] \times A \in \RR^{q \times p}.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Universality}
\label{sec-universality}

In this section, to ease the exposition, we explicitly introduce the bias and use the variable ``$x \in \RR^p$'' in place of ``$a \in \RR^p$''. We thus write the function computed by the MLP (including explicitly the bias $z_k$) as 
\eq{
	h_{W,z,u}(x) \eqdef \sum_{k=1}^q u_k \phi_{w_k,z_k}(x)
	\qwhereq
	\phi_{w,z}(x) \eqdef \rho( \dotp{x}{w}+z ).
}
The function $\phi_{w,z}(x)$ is a ridge function in the direction orthogonal to $\bar w \eqdef w/\norm{w}$ and passing around the point $-\frac{z}{\norm{w}}\bar w$.

In the following we assume that $\rho : \RR \rightarrow \RR$ is a bounded function such that 
\eql{\label{eq-constraint-univ}
	\rho(r) \overset{r \rightarrow -\infty}{\longrightarrow} 0
	\qandq
	\rho(r) \overset{r \rightarrow +\infty}{\longrightarrow} 1.
}
Note in particular that such a function cannot be a polynomial and that the ReLu function does not satisfy these hypothesis (universality for the ReLu is more involved to show).
%
The goal is to show the following theorem.

\begin{thm}[Cybenko, 1989]\label{thm-universality}
	For any compact set $\Om \subset \RR^p$, the space spanned by the functions $\{\phi_{w,z}\}_{w,z}$ is dense in $\Cc(\Om)$ for the uniform convergence. This means that for any continuous function $f$ and any $\epsilon>0$, there exists $q \in \NN$ and weights $(w_k,z_k,u_k)_{k=1}^q$ such that
	\eq{
		\foralls x \in \Om, \quad
		|f(x) - \sum_{k=1}^q u_k \phi_{w_k,z_k}(x)| \leq \epsilon.
	}
\end{thm}

In a typical ML scenario, this implies that one can ``overfit'' the data, since using a $q$ large enough ensures that the training error can be made arbitrary small. Of course, there is a bias-variance tradeoff, and $q$ needs to be cross-validated to account for the finite number $n$ of data, and ensure a good generalization properties.  

%%%
\paragraph{Proof in dimension $p=1$.}

In 1D, the approximation $h_{W,z,u}$ can be thought as an approximation using smoothed step functions. Indeed, introducing a parameter $\epsilon>0$, one has (assuming the function is Lipschitz to ensure uniform convergence), 
\eq{
	\phi_{\frac{w}{\epsilon},\frac{z_k}{\epsilon}} \overset{\epsilon \rightarrow 0}{\longrightarrow} 1_{[-z/w,+\infty[}
}
This means that 
\eq{
	h_{\frac{W}{\epsilon},\frac{z}{\epsilon},u} \overset{\epsilon \rightarrow 0}{\longrightarrow} \sum_k u_k 1_{[-z_k/w_k,+\infty[}, 
}
which is a piecewise constant function. Inversely, any piecewise constant function can be written this way.
%
Indeed, if $h$ assumes the value $d_k$ on each interval $[t_k,t_{k+1}[$, then it can be written as
\eq{
	h = \sum_k d_k ( 1_{[t_k,+\infty[} - 1_{[t_k,+\infty[} ).
}
Since the space of piecewise constant functions is dense in continuous function over an interval, this proves the theorem.

%%%
\paragraph{Proof in arbitrary dimension $p$.}

We start by proving the following dual characterization of density, using bounded Borel measure $\mu \in \Mm(\Om)$ i.e. such that $\mu(\Om)<+\infty$.

\begin{prop}\label{prop-proof-univ-1}
	If $\rho$ is such that for any Borel measure $\mu \in \Mm(\Om)$
	\eql{\label{eq-prop-proof-univ-1}
		\pa{
			\foralls (w,z), \int \rho(\dotp{x}{w}+z) \d \mu(x) = 0
		}
		\qarrq 
		\mu = 0, 
	}
	then Theorem~\ref{thm-universality} holds. 
\end{prop}

\begin{proof}
	We consider the linear space
	\eq{
		\Ss \eqdef \enscond{ \sum_{k=1}^q u_k \phi_{w_k,z_k} }{q \in \NN, w_k \in \RR^p, u_k \in \RR, z_k \in \RR } \subset \Cc(\Om).
	}
	Let $\bar\Ss$ be its closure in $\Cc(\Om)$ for $\norm{\cdot}_\infty$, which is a Banach space.
	%
	If $\bar\Ss \neq \Cc(\Om)$, let us pick $g \neq 0$, $g \in \Cc(\Om) \backslash \bar\Ss$.
	%
	We define the linear form $L$ on $\bar\Ss \oplus \text{span}(g)$ as
	\eq{
		\foralls s \in \bar\Ss, \: \foralls \la \in \RR, \quad
		\quad L(s+\la g) = \la
	}
	so that $L=0$ on $\bar\Ss$. $L$ is a bounded linear form, so that by Hahn-Banach theorem, it can be extended in a bounded linear form $\bar L : \Cc(\Om) \rightarrow \RR$. Since $L \in \Cc(\Om)^*$ (the dual space of continuous linear form), and that this dual space is identified with Borel measures, there exists $\mu \in \Mm(\Om)$, with $\mu \neq 0$, such that for any continuous function $h$, $\bar L(h) = \int_\Om h(x) \d\mu(x)$.
	%
	But since $\bar L = 0$ on $\bar\Ss$, $\int \rho(\dotp{\cdot}{w}+z) \d \mu = 0$ for all $(w,z)$ and thus by hypothesis, $\mu=0$, which is a contradiction.
\end{proof}

The theorem now follows from the following proposition.

\begin{prop}
	If $\rho$ is continuous and satisfies~\eqref{eq-constraint-univ}, then it satisfies~\eqref{eq-prop-proof-univ-1}.
\end{prop}

\begin{proof}
	One has
	\eq{
		\phi_{\frac{w}{\epsilon}, \frac{u}{\epsilon}+t}(x)
		= \rho\pa{ \frac{\dotp{x}{w}+u}{\epsilon} + t }
		%
		\overset{\epsilon \rightarrow 0}{\longrightarrow}
		\ga(x) \eqdef 
		\choice{
			1 \qifq H_{w,u},   \\ 
			\rho(t) \qifq x \in P_{w,u},  \\
			0 \qifq \dotp{w}{x}+u < 0,  \\ 
		}
	}
	where we defined $H_{w,u} \eqdef \enscond{x}{\dotp{w}{x}+u > 0}$ and
	$P_{w,u} \eqdef \enscond{x}{\dotp{w}{x}+u = 0}$.
	%
	By Lebesgue dominated convergence (since the involved quantities are bounded uniformly on a compact set)
	\eq{
		\int \phi_{\frac{w}{\epsilon}, \frac{u}{\epsilon}+t} \d \mu		
		%
		\overset{\epsilon \rightarrow 0}{\longrightarrow}
		%
		\int \ga \d\mu = \phi(t) \mu(P_{w,u}) + \mu(H_{w,u}).
	}
	Thus if $\mu$ is such that all these integrals vanish, then 
	\eq{
		\foralls (w,u,t), \quad \phi(t) \mu(P_{w,u}) + \mu(H_{w,u}) = 0.
	}
	By selecting $(t,t')$ such that $\phi(t) \neq \phi(t')$, one has that 
	\eq{
		\foralls (w,u), \quad  \mu(P_{w,u}) = \mu(H_{w,u}) = 0.
	}
	We now need to show that $\mu=0$.  For a fixed $w \in \RR^p$, we consider the function 
	\eq{
		h \in L^\infty(\RR), \quad
		F(h) \eqdef \int_\Om h(\dotp{w}{x}) \d \mu(x).
	}
	$F : L^\infty(\RR) \rightarrow \RR$ is a bounded linear form since $|F(\mu)| \leq \norm{h}_\infty \mu(\Om)$ and $\mu(\Om) < +\infty$. One has
	\eq{
		F(1_{[-u,+\infty[} = \int_{\Om} 1_{[-u,+\infty[}(\dotp{w}{x}) \d \mu(x)
		=  \mu( P_{w,u} ) + \mu( H_{w,u} ) = 0.
 	} 
	By linearity, $F(h)=0$ for all piecewise constant functions, and $F$ is a continuous linear form, so that by density $F(h)=0$ for all functions $h \in L^\infty(\RR)$. Applying this for $h(r)=e^{\imath r}$ one obtains
	\eq{
		\hat \mu(w) \eqdef \int_\Om e^{\imath \dotp{x}{w}} \d \mu(x) = 0.
	} 
	This means that the Fourier transform of $\mu$ is zero, so that $\mu=0$.
\end{proof}

%%%
\paragraph{Quantitative rates.}

Note that Theorem~\ref{thm-universality} is not constructive in the sense that it does not explain how to compute the weights $(w_k,u_k,z_k)_k$ to reach a desired accuracy. Since for a fixed $q$ the function is non-convex, this is not surprising. Some recent studies show that if $q$ is large enough, a simple gradient descent is able to reach an arbitrary good accuracy, but it might require a very large $q$.

Theorem~\ref{thm-universality} is also not quantitative since it does not tell how much neurons $q$ is needed to reach a desired accuracy. To obtain quantitative bounds, continuity is not enough, it requires to add smoothness constraints. For instance, Barron proved that if 
\eq{
	\int \norm{\om} |\hat f(\om)| \d\om \leq C_f
}
where $\hat f(\om) = \int f(x) e^{-\imath\dotp{x}{\om}} \d x$ is the Fourier transform of $f$, then for $q \in \NN$ there exists $(w_k,u_k,z_k)_{k}$
\eq{
	\frac{1}{\text{Vol}(B(0,r))} \int_{\norm{x} \leq r} |f(x) - \sum_{k=1}^q u_k \phi_{w_k,z_k}(x)|^2 \d x\leq \frac{(2rC_f)^2}{q}.
}
The surprising part of this Theorem is that the $1/q$ decay is independent of the dimension $p$. Note however that the constant involved $C_f$ might depend on $p$. 

