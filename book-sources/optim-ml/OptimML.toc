\contentsline {section}{\numberline {1}Motivation in Machine Learning}{2}{section.1}
\contentsline {subsection}{\numberline {1.1}Unconstraint optimization}{2}{subsection.1.1}
\contentsline {subsection}{\numberline {1.2}Regression}{3}{subsection.1.2}
\contentsline {subsection}{\numberline {1.3}Classification}{3}{subsection.1.3}
\contentsline {section}{\numberline {2}Basics of Convex Analysis}{3}{section.2}
\contentsline {subsection}{\numberline {2.1}Existence of Solutions}{3}{subsection.2.1}
\contentsline {subsection}{\numberline {2.2}Convexity}{4}{subsection.2.2}
\contentsline {paragraph}{Strict convexity.}{5}{section*.2}
\contentsline {subsection}{\numberline {2.3}Convex Sets}{5}{subsection.2.3}
\contentsline {section}{\numberline {3}Derivative and gradient}{5}{section.3}
\contentsline {subsection}{\numberline {3.1}Gradient}{5}{subsection.3.1}
\contentsline {subsection}{\numberline {3.2}First Order Conditions}{6}{subsection.3.2}
\contentsline {subsection}{\numberline {3.3}Least Squares}{7}{subsection.3.3}
\contentsline {subsection}{\numberline {3.4}Link with PCA}{8}{subsection.3.4}
\contentsline {subsection}{\numberline {3.5}Classification}{9}{subsection.3.5}
\contentsline {subsection}{\numberline {3.6}Chain Rule}{9}{subsection.3.6}
\contentsline {section}{\numberline {4}Gradient Descent Algorithm}{10}{section.4}
\contentsline {subsection}{\numberline {4.1}Steepest Descent Direction}{10}{subsection.4.1}
\contentsline {subsection}{\numberline {4.2}Gradient Descent}{11}{subsection.4.2}
\contentsline {section}{\numberline {5}Convergence Analysis}{12}{section.5}
\contentsline {subsection}{\numberline {5.1}Quadratic Case}{12}{subsection.5.1}
\contentsline {paragraph}{Convergence analysis for the quadratic case.}{12}{section*.3}
\contentsline {subsection}{\numberline {5.2}General Case}{15}{subsection.5.2}
\contentsline {paragraph}{Hessian.}{15}{section*.4}
\contentsline {paragraph}{Smoothness and strong convexity.}{16}{section*.5}
\contentsline {paragraph}{Convergence analysis.}{17}{section*.6}
\contentsline {section}{\numberline {6}Regularization}{18}{section.6}
\contentsline {subsection}{\numberline {6.1}Penalized Least Squares}{18}{subsection.6.1}
\contentsline {subsection}{\numberline {6.2}Ridge Regression}{19}{subsection.6.2}
\contentsline {paragraph}{Pseudo-inverse.}{19}{section*.7}
\contentsline {subsection}{\numberline {6.3}Lasso}{19}{subsection.6.3}
\contentsline {subsection}{\numberline {6.4}Iterative Soft Thresholding}{21}{subsection.6.4}
\contentsline {section}{\numberline {7}Stochastic Optimization}{21}{section.7}
\contentsline {subsection}{\numberline {7.1}Minimizing Sums and Expectation}{22}{subsection.7.1}
\contentsline {subsection}{\numberline {7.2}Batch Gradient Descent (BGD)}{22}{subsection.7.2}
\contentsline {subsection}{\numberline {7.3}Stochastic Gradient Descent (SGD)}{23}{subsection.7.3}
\contentsline {subsection}{\numberline {7.4}Stochastic Gradient Descent with Averaging (SGA)}{25}{subsection.7.4}
\contentsline {subsection}{\numberline {7.5}Stochastic Averaged Gradient Descent (SAG)}{26}{subsection.7.5}
\contentsline {section}{\numberline {8}Multi-Layers Perceptron}{26}{section.8}
\contentsline {subsection}{\numberline {8.1}MLP and its derivative}{26}{subsection.8.1}
\contentsline {paragraph}{Expressiveness. }{27}{section*.8}
\contentsline {subsection}{\numberline {8.2}MLP and Gradient Computation}{27}{subsection.8.2}
\contentsline {paragraph}{Optimizing with respect to $u$.}{27}{section*.9}
\contentsline {paragraph}{Optimizing with respect to $W$.}{28}{section*.10}
\contentsline {subsection}{\numberline {8.3}Universality}{28}{subsection.8.3}
\contentsline {paragraph}{Proof in dimension $p=1$.}{29}{section*.11}
\contentsline {paragraph}{Proof in arbitrary dimension $p$.}{29}{section*.12}
\contentsline {paragraph}{Quantitative rates.}{30}{section*.13}
\contentsline {section}{\numberline {9}Automatic Differentiation}{30}{section.9}
\contentsline {subsection}{\numberline {9.1}Finite Differences and Symbolic Calculus}{31}{subsection.9.1}
\contentsline {subsection}{\numberline {9.2}Computational Graphs}{31}{subsection.9.2}
\contentsline {subsection}{\numberline {9.3}Forward Mode of Automatic Differentiation}{31}{subsection.9.3}
\contentsline {paragraph}{Simple example.}{32}{section*.14}
\contentsline {paragraph}{Dual numbers.}{33}{section*.15}
\contentsline {subsection}{\numberline {9.4}Reverse Mode of Automatic Differentiation}{34}{subsection.9.4}
\contentsline {paragraph}{Back-propagation.}{34}{section*.16}
\contentsline {paragraph}{Simple example.}{34}{section*.17}
\contentsline {subsection}{\numberline {9.5}Feed-forward Compositions}{35}{subsection.9.5}
\contentsline {subsection}{\numberline {9.6}Feed-forward Architecture}{35}{subsection.9.6}
\contentsline {paragraph}{Multilayers perceptron.}{36}{section*.18}
\contentsline {paragraph}{Link with adjoint state method.}{36}{section*.19}
\contentsline {subsection}{\numberline {9.7}Recurrent Architectures}{37}{subsection.9.7}
\contentsline {paragraph}{Recurrent networks. }{37}{section*.20}
\contentsline {paragraph}{Mitigating memory requirement. }{37}{section*.21}
\contentsline {paragraph}{Fixed point maps}{38}{section*.22}
\contentsline {paragraph}{Argmin layers}{38}{section*.23}
\contentsline {paragraph}{Sinkhorn's algorithm}{39}{section*.24}
